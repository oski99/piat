{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word_to_id mapping from a file\n",
    "with open('utils/word_to_id_mapping.pkl', 'rb') as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "\n",
    "# load the id_to_word mapping from a file\n",
    "with open('utils/id_to_word_mapping.pkl', 'rb') as f:\n",
    "    id_to_word = pickle.load(f)\n",
    "    \n",
    "# load the tokenized sentences from a file\n",
    "with open('utils/tokenized_sentences.pkl', 'rb') as f:\n",
    "    tokenized_sentences = pickle.load(f)\n",
    "\n",
    "# load the sentences from a file\n",
    "with open('utils/sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 40, 128)           6400128   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 50001)             6450129   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,981,841\n",
      "Trainable params: 12,981,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "hidden_units = 128\n",
    "max_sentence_length = 40\n",
    "\n",
    "# Define the model\n",
    "def build_lstm_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sentence_length))\n",
    "    model.add(LSTM(units=hidden_units))\n",
    "    model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Calculate vocabulary size\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# Build the model\n",
    "model = build_lstm_model(vocab_size)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_checkpoint_{epoch:02d}.h5', save_freq=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        sentence = ['dawno', 'dawno', 'temu']  \n",
    "        seed_context = [word_to_id[word] for word in sentence if word in word_to_id]\n",
    "\n",
    "        num_words_to_generate = 20  \n",
    "\n",
    "        sentence_added = 'dawno dawno temu'\n",
    "        for _ in range(num_words_to_generate):\n",
    "            padded_context = np.array([seed_context + [word_to_id['<PAD>']] * (max_sentence_length - len(seed_context))])\n",
    "            predicted_probs = self.model.predict(padded_context, verbose=0)\n",
    "            predicted_word_id = np.argmax(predicted_probs)\n",
    "            predicted_word = id_to_word[predicted_word_id]\n",
    "            sentence_added += \" \" + predicted_word\n",
    "            seed_context.append(predicted_word_id)\n",
    "            seed_context = seed_context[-max_sentence_length:] \n",
    "        print()\n",
    "        print(sentence_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 8.3671 - accuracy: 0.0295\n",
      "dawno dawno temu się się i i i i i i i i i i i i i i i i i i\n",
      "7276/7276 [==============================] - 1244s 171ms/step - loss: 8.3671 - accuracy: 0.0295\n",
      "Epoch 2/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 8.0869 - accuracy: 0.0377\n",
      "dawno dawno temu nie nie nie nie nie nie nie nie nie nie nie nie nie nie nie nie nie nie nie nie\n",
      "7276/7276 [==============================] - 1253s 172ms/step - loss: 8.0869 - accuracy: 0.0377\n",
      "Epoch 3/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.9342 - accuracy: 0.0445\n",
      "dawno dawno temu się i i nie ma się i i nie ma się i i nie ma się i i nie ma\n",
      "7276/7276 [==============================] - 1241s 171ms/step - loss: 7.9342 - accuracy: 0.0445\n",
      "Epoch 4/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.8857 - accuracy: 0.0500\n",
      "dawno dawno temu się w i i i nie ma się i i nie ma się i i i i i i i\n",
      "7276/7276 [==============================] - 1248s 171ms/step - loss: 7.8857 - accuracy: 0.0500\n",
      "Epoch 5/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.8365 - accuracy: 0.0547\n",
      "dawno dawno temu nie ma się w tej chwili i nie ma się w tej chwili w tej chwili w tej chwili w\n",
      "7276/7276 [==============================] - 1253s 172ms/step - loss: 7.8365 - accuracy: 0.0547\n",
      "Epoch 6/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.7702 - accuracy: 0.0615\n",
      "dawno dawno temu nie ma się w tym nie ma i nie ma nie ma i nie ma i nie ma i nie\n",
      "7276/7276 [==============================] - 1250s 172ms/step - loss: 7.7702 - accuracy: 0.0615\n",
      "Epoch 7/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.7270 - accuracy: 0.0668\n",
      "dawno dawno temu nie było to nie było i nie było w tym chwili nie było i w tym chwili że nie było\n",
      "7276/7276 [==============================] - 1223s 168ms/step - loss: 7.7270 - accuracy: 0.0668\n",
      "Epoch 8/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.6955 - accuracy: 0.0700\n",
      "dawno dawno temu nie ma być nie ma być nie ma być nie ma być nie ma być w tym nie ma być\n",
      "7276/7276 [==============================] - 1224s 168ms/step - loss: 7.6955 - accuracy: 0.0700\n",
      "Epoch 9/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.6404 - accuracy: 0.0733\n",
      "dawno dawno temu nie było w tym chwili nie było w tym chwili w tym chwili nie było w tym nie było w\n",
      "7276/7276 [==============================] - 1220s 168ms/step - loss: 7.6404 - accuracy: 0.0733\n",
      "Epoch 10/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.5932 - accuracy: 0.0762\n",
      "dawno dawno temu nie ma się w tym nie było to i nie było w tym nie było w tym nie było w\n",
      "7276/7276 [==============================] - 1212s 167ms/step - loss: 7.5932 - accuracy: 0.0762\n",
      "Epoch 11/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.5482 - accuracy: 0.0793\n",
      "dawno dawno temu nie ma się na to i nie było i nie było i nie było w tym co było w tym\n",
      "7276/7276 [==============================] - 1217s 167ms/step - loss: 7.5482 - accuracy: 0.0793\n",
      "Epoch 12/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.4833 - accuracy: 0.0808\n",
      "dawno dawno temu nie ma i nie ma i nie ma być w tej chwili i nie ma być w tej chwili i\n",
      "7276/7276 [==============================] - 1217s 167ms/step - loss: 7.4833 - accuracy: 0.0808\n",
      "Epoch 13/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.4886 - accuracy: 0.0821\n",
      "dawno dawno temu nie mógł się w tym wątpliwości że nie było w tym wątpliwości że nie było w tym wątpliwości że nie\n",
      "7276/7276 [==============================] - 1216s 167ms/step - loss: 7.4886 - accuracy: 0.0821\n",
      "Epoch 14/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.4226 - accuracy: 0.0846\n",
      "dawno dawno temu nie ma tego czasu i nie było w tym czasie nie było w tym czasie że nie było w tym\n",
      "7276/7276 [==============================] - 1222s 168ms/step - loss: 7.4226 - accuracy: 0.0846\n",
      "Epoch 15/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.4096 - accuracy: 0.0857\n",
      "dawno dawno temu nie było w tym co się stało i nie było w tym co się stało i w tym co się\n",
      "7276/7276 [==============================] - 1225s 168ms/step - loss: 7.4096 - accuracy: 0.0857\n",
      "Epoch 16/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.3715 - accuracy: 0.0868\n",
      "dawno dawno temu nie było to co się stało i nie było w tym co się stało i nie było w tym miejscu\n",
      "7276/7276 [==============================] - 1226s 168ms/step - loss: 7.3715 - accuracy: 0.0868\n",
      "Epoch 17/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.3573 - accuracy: 0.0880\n",
      "dawno dawno temu się nie ma i nie było to i nie było w tym czasie i nie było się w tym miejscu\n",
      "7276/7276 [==============================] - 1223s 168ms/step - loss: 7.3573 - accuracy: 0.0880\n",
      "Epoch 18/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.3299 - accuracy: 0.0887\n",
      "dawno dawno temu nie się w tym miejscu w tym co się stało i nie ma ani słowa nie się w tym miejscu\n",
      "7276/7276 [==============================] - 1226s 168ms/step - loss: 7.3299 - accuracy: 0.0887\n",
      "Epoch 19/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.2929 - accuracy: 0.0907\n",
      "dawno dawno temu nie ma w tym co się stało i nie było w tym razie i nie było w tym razie i\n",
      "7276/7276 [==============================] - 1224s 168ms/step - loss: 7.2929 - accuracy: 0.0907\n",
      "Epoch 20/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.2730 - accuracy: 0.0914\n",
      "dawno dawno temu nie ma tego czasu do tego czasu i nie było tego czasu do domu i nie było w tym miejscu\n",
      "7276/7276 [==============================] - 1226s 169ms/step - loss: 7.2730 - accuracy: 0.0914\n",
      "Epoch 21/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.2566 - accuracy: 0.0923\n",
      "dawno dawno temu nie ma żadnych wyjaśnień i nie ma żadnego rodzaju nie było żadnego rodzaju i nie było żadnego rodzaju i nie\n",
      "7276/7276 [==============================] - 1220s 168ms/step - loss: 7.2566 - accuracy: 0.0923\n",
      "Epoch 22/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.2336 - accuracy: 0.0928\n",
      "dawno dawno temu nie ma nic za to że nie ma czasu do czasu do czasu do czasu do czasu do czasu do\n",
      "7276/7276 [==============================] - 1235s 170ms/step - loss: 7.2336 - accuracy: 0.0928\n",
      "Epoch 23/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.2176 - accuracy: 0.0942\n",
      "dawno dawno temu nie ma co robić do tego nie ma sposobu do tego i nie było się w tym miejscu i z\n",
      "7276/7276 [==============================] - 1231s 169ms/step - loss: 7.2176 - accuracy: 0.0942\n",
      "Epoch 24/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.1853 - accuracy: 0.0962\n",
      "dawno dawno temu nie ma w tym samym miejscu i nie było w tej chwili nie było w tej chwili nie było w\n",
      "7276/7276 [==============================] - 1230s 169ms/step - loss: 7.1853 - accuracy: 0.0962\n",
      "Epoch 25/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.1606 - accuracy: 0.0962\n",
      "dawno dawno temu nie mógł się dowiedzieć o tym że nie ma nic do tego że nie było w tym wątpliwości że nie\n",
      "7276/7276 [==============================] - 1235s 170ms/step - loss: 7.1606 - accuracy: 0.0962\n",
      "Epoch 26/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.1669 - accuracy: 0.0974\n",
      "dawno dawno temu się w tym miejscu nie było w tym miejscu i się w tym miejscu i się w tym miejscu i\n",
      "7276/7276 [==============================] - 1232s 169ms/step - loss: 7.1669 - accuracy: 0.0974\n",
      "Epoch 27/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.1476 - accuracy: 0.0980\n",
      "dawno dawno temu nie ma w tym wypadku nie było w tym bardziej że nie ma czasu do tego czasu do tego stopnia\n",
      "7276/7276 [==============================] - 1231s 169ms/step - loss: 7.1476 - accuracy: 0.0980\n",
      "Epoch 28/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.1269 - accuracy: 0.0978\n",
      "dawno dawno temu nie mógł się domyślić z nim w tym że nie było w tym wyobrazić że nie było w tym wyobrazić\n",
      "7276/7276 [==============================] - 1239s 170ms/step - loss: 7.1269 - accuracy: 0.0978\n",
      "Epoch 29/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.1047 - accuracy: 0.0993\n",
      "dawno dawno temu nie ma w tej chwili nie było to co się w nim z tego w tej części i w jego\n",
      "7276/7276 [==============================] - 1238s 170ms/step - loss: 7.1047 - accuracy: 0.0993\n",
      "Epoch 30/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.0841 - accuracy: 0.1001\n",
      "dawno dawno temu się nie mogąc nie mogąc się domyśleć o tym nie ma nic nie ma do mnie i nie mógł się\n",
      "7276/7276 [==============================] - 1237s 170ms/step - loss: 7.0841 - accuracy: 0.1001\n",
      "Epoch 31/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.0684 - accuracy: 0.1015\n",
      "dawno dawno temu nie ma prawa odpowiedzieć i nie było nic innego że nie było ani słowa ani ani śladu ani ani śladu\n",
      "7276/7276 [==============================] - 1237s 170ms/step - loss: 7.0684 - accuracy: 0.1015\n",
      "Epoch 32/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.0619 - accuracy: 0.1014\n",
      "dawno dawno temu że nie ma rady i nie ma rady i nie było to co się stało i nie ma się do\n",
      "7276/7276 [==============================] - 1235s 170ms/step - loss: 7.0619 - accuracy: 0.1014\n",
      "Epoch 33/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.0517 - accuracy: 0.1024\n",
      "dawno dawno temu nie mógł się powstrzymać w głowie i nie było ani słowa ani słowa z nim nie mógł się powstrzymać w\n",
      "7276/7276 [==============================] - 1236s 170ms/step - loss: 7.0517 - accuracy: 0.1024\n",
      "Epoch 34/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.0399 - accuracy: 0.1032\n",
      "dawno dawno temu nie mógł się z nim dostać na to się z sobą i nie mógł się z nim pokazywać w kierunku\n",
      "7276/7276 [==============================] - 1238s 170ms/step - loss: 7.0399 - accuracy: 0.1032\n",
      "Epoch 35/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 7.0248 - accuracy: 0.1030 \n",
      "dawno dawno temu nie ma na świecie i nie ma nic innego niż na to nie ma nic innego niż na to nie\n",
      "7276/7276 [==============================] - 113649s 16s/step - loss: 7.0248 - accuracy: 0.1030\n",
      "Epoch 36/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.9962 - accuracy: 0.1047\n",
      "dawno dawno temu nie było w tym że nie było czasu do tego czasu nie było żadnego śladu i na to było w\n",
      "7276/7276 [==============================] - 1219s 168ms/step - loss: 6.9962 - accuracy: 0.1047\n",
      "Epoch 37/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.9996 - accuracy: 0.1045\n",
      "dawno dawno temu nie ma w tym że nie ma nic do tego że nie ma do czynienia z tego stopnia że nie\n",
      "7276/7276 [==============================] - 1288s 177ms/step - loss: 6.9996 - accuracy: 0.1045\n",
      "Epoch 38/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.9813 - accuracy: 0.1047\n",
      "dawno dawno temu i nie ma czasu do tego czasu i nie było w tej chwili nie było w tej chwili i nie\n",
      "7276/7276 [==============================] - 1329s 183ms/step - loss: 6.9813 - accuracy: 0.1047\n",
      "Epoch 39/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.9502 - accuracy: 0.1069\n",
      "dawno dawno temu nie ma w tej chwili nie było w tej chwili nie było w tej chwili nie było w tej chwili\n",
      "7276/7276 [==============================] - 1229s 169ms/step - loss: 6.9502 - accuracy: 0.1069\n",
      "Epoch 40/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.9584 - accuracy: 0.1059\n",
      "dawno dawno temu nie ma się w tym wyobrazić że nie ma ani trudności ani ani trudności ani ani trudności ani ani trudności\n",
      "7276/7276 [==============================] - 1229s 169ms/step - loss: 6.9584 - accuracy: 0.1059\n",
      "Epoch 41/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.9325 - accuracy: 0.1074\n",
      "dawno dawno temu nie ma w tej chwili nie było ani śladu ani śladu ani ani ani ani ani ani ani ani ani\n",
      "7276/7276 [==============================] - 1224s 168ms/step - loss: 6.9325 - accuracy: 0.1074\n",
      "Epoch 42/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.9422 - accuracy: 0.1059\n",
      "dawno dawno temu nie ma co do mnie na to i nie było co do mnie i nie było na świecie i nie\n",
      "7276/7276 [==============================] - 1227s 169ms/step - loss: 6.9422 - accuracy: 0.1059\n",
      "Epoch 43/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.9141 - accuracy: 0.1083\n",
      "dawno dawno temu nie ma w niej i nie było nic innego niż w tej chwili nie było nic do tego czasu do\n",
      "7276/7276 [==============================] - 1231s 169ms/step - loss: 6.9141 - accuracy: 0.1083\n",
      "Epoch 44/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.9028 - accuracy: 0.1076\n",
      "dawno dawno temu nie ma w tej chwili nie było ani słowa ani ani ani ani ani ani ani ani ani ani ani\n",
      "7276/7276 [==============================] - 1230s 169ms/step - loss: 6.9028 - accuracy: 0.1076\n",
      "Epoch 45/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8950 - accuracy: 0.1077\n",
      "dawno dawno temu nie ma w tym znaczeniu nie ma w życiu w tym że nie ma czasu do tego stopnia że nie\n",
      "7276/7276 [==============================] - 1227s 169ms/step - loss: 6.8950 - accuracy: 0.1077\n",
      "Epoch 46/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8845 - accuracy: 0.1083\n",
      "dawno dawno temu nie ma w świecie nie było już nic do stracenia nie było nic nie było do czynienia z powrotem do\n",
      "7276/7276 [==============================] - 1230s 169ms/step - loss: 6.8845 - accuracy: 0.1083\n",
      "Epoch 47/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8654 - accuracy: 0.1105\n",
      "dawno dawno temu nie może być nie może być może że nie może być nie może być nie może być nie może powiedzieć\n",
      "7276/7276 [==============================] - 1232s 169ms/step - loss: 6.8654 - accuracy: 0.1105\n",
      "Epoch 48/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8760 - accuracy: 0.1099\n",
      "dawno dawno temu nie ma w tej chwili na to się z nim do niego i nie było w tej chwili z nim\n",
      "7276/7276 [==============================] - 1223s 168ms/step - loss: 6.8760 - accuracy: 0.1099\n",
      "Epoch 49/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8474 - accuracy: 0.1109\n",
      "dawno dawno temu nie ma w tym że nie ma żadnego nadziei że nie ma w tym że nie ma żadnego nadziei że\n",
      "7276/7276 [==============================] - 1225s 168ms/step - loss: 6.8474 - accuracy: 0.1109\n",
      "Epoch 50/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8515 - accuracy: 0.1119\n",
      "dawno dawno temu nie mógł się przekonać że nie ma odwagi i nie było ani śladu ani ani śladu i w tym czasie\n",
      "7276/7276 [==============================] - 1225s 168ms/step - loss: 6.8515 - accuracy: 0.1119\n",
      "Epoch 51/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8445 - accuracy: 0.1110\n",
      "dawno dawno temu nie wiedział co się dzieje w duszy i nie ma czasu do stracenia rzekł pan zagłoba i nie wiem co\n",
      "7276/7276 [==============================] - 1226s 169ms/step - loss: 6.8445 - accuracy: 0.1110\n",
      "Epoch 52/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8436 - accuracy: 0.1135\n",
      "dawno dawno temu nie ma w niej nie się w tym bardziej że nie ma w niej nie się w tym bardziej że\n",
      "7276/7276 [==============================] - 1218s 167ms/step - loss: 6.8436 - accuracy: 0.1135\n",
      "Epoch 53/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8095 - accuracy: 0.1134\n",
      "dawno dawno temu nie ma w tym niż to nie było na świecie i nie było w tym że nie było w tym\n",
      "7276/7276 [==============================] - 1225s 168ms/step - loss: 6.8095 - accuracy: 0.1134\n",
      "Epoch 54/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8165 - accuracy: 0.1126\n",
      "dawno dawno temu nie ma w niej i to nie było to i to nie było to i nie było w tej chwili\n",
      "7276/7276 [==============================] - 1221s 168ms/step - loss: 6.8165 - accuracy: 0.1126\n",
      "Epoch 55/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8026 - accuracy: 0.1128\n",
      "dawno dawno temu nie ma w tym niż nie ma nic innego niż na to nie było nic do tego niż nie było\n",
      "7276/7276 [==============================] - 1222s 168ms/step - loss: 6.8026 - accuracy: 0.1128\n",
      "Epoch 56/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.8051 - accuracy: 0.1132\n",
      "dawno dawno temu nie ma w nim i nie ma w tej chwili i nie było w tej chwili i nie było w\n",
      "7276/7276 [==============================] - 1219s 167ms/step - loss: 6.8051 - accuracy: 0.1132\n",
      "Epoch 57/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7903 - accuracy: 0.1131\n",
      "dawno dawno temu nie ma w tej chwili nie było ani słowa ani ani ani ani ani ani ani ani ani ani ani\n",
      "7276/7276 [==============================] - 1223s 168ms/step - loss: 6.7903 - accuracy: 0.1131\n",
      "Epoch 58/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7808 - accuracy: 0.1135\n",
      "dawno dawno temu nie ma w tej chwili nie było w tej chwili nie było w tej chwili i na to nie było\n",
      "7276/7276 [==============================] - 1226s 169ms/step - loss: 6.7808 - accuracy: 0.1135\n",
      "Epoch 59/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7723 - accuracy: 0.1153\n",
      "dawno dawno temu nie widział w tym względzie nie było ani słowa ani słowa ani słowa ani słowa ani słowa ani ani ani\n",
      "7276/7276 [==============================] - 1238s 170ms/step - loss: 6.7723 - accuracy: 0.1153\n",
      "Epoch 60/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7638 - accuracy: 0.1153\n",
      "dawno dawno temu nie ma w nim z tego stopnia że nie było w tym że nie było w tym że nie było\n",
      "7276/7276 [==============================] - 1453s 200ms/step - loss: 6.7638 - accuracy: 0.1153\n",
      "Epoch 61/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7643 - accuracy: 0.1153\n",
      "dawno dawno temu nie ma w niej myśleć o tym że nie ma już nic więcej niż to nie jest to niemożliwe odparł\n",
      "7276/7276 [==============================] - 1362s 187ms/step - loss: 6.7643 - accuracy: 0.1153\n",
      "Epoch 62/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7485 - accuracy: 0.1157\n",
      "dawno dawno temu nie ma w tym i nie mogę się przekonać czy nie ma w tym że nie ma nic podobnego do\n",
      "7276/7276 [==============================] - 1225s 168ms/step - loss: 6.7485 - accuracy: 0.1157\n",
      "Epoch 63/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7513 - accuracy: 0.1155\n",
      "dawno dawno temu nie ma nic więcej niż to nie było w tym czasie i nie było w tym rodzaju i i w\n",
      "7276/7276 [==============================] - 1217s 167ms/step - loss: 6.7513 - accuracy: 0.1155\n",
      "Epoch 64/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7259 - accuracy: 0.1176\n",
      "dawno dawno temu nie będzie w tym rodzaju i i i i i i i i i i i i i i i\n",
      "7276/7276 [==============================] - 1232s 169ms/step - loss: 6.7259 - accuracy: 0.1176\n",
      "Epoch 65/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7365 - accuracy: 0.1174\n",
      "dawno dawno temu nie ma co robić do niego i nie ma co robić do niego i nie ma co robić do niego\n",
      "7276/7276 [==============================] - 1226s 168ms/step - loss: 6.7365 - accuracy: 0.1174\n",
      "Epoch 66/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7274 - accuracy: 0.1173\n",
      "dawno dawno temu nie ma nic więcej niż to w tej chwili nie było żadnej wątpliwości że nie mógł się spodziewać że nie\n",
      "7276/7276 [==============================] - 1226s 168ms/step - loss: 6.7274 - accuracy: 0.1173\n",
      "Epoch 67/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7287 - accuracy: 0.1172\n",
      "dawno dawno temu nie ma nic innego jak i i nie się w tym że nie ma nic innego jak i i i\n",
      "7276/7276 [==============================] - 1226s 169ms/step - loss: 6.7287 - accuracy: 0.1172\n",
      "Epoch 68/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7216 - accuracy: 0.1168\n",
      "dawno dawno temu nie mógł się powstrzymać od granic i z sobą i i i i i i i i i i i\n",
      "7276/7276 [==============================] - 1225s 168ms/step - loss: 6.7216 - accuracy: 0.1168\n",
      "Epoch 69/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7133 - accuracy: 0.1170\n",
      "dawno dawno temu nie będzie w spychowie i i nie ma się w tym że nie ma się w tym że nie było\n",
      "7276/7276 [==============================] - 1229s 169ms/step - loss: 6.7133 - accuracy: 0.1170\n",
      "Epoch 70/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7004 - accuracy: 0.1184\n",
      "dawno dawno temu nie ma nic innego jak i nie ma nic innego jak w tej chwili nie było w tej chwili że\n",
      "7276/7276 [==============================] - 1226s 169ms/step - loss: 6.7004 - accuracy: 0.1184\n",
      "Epoch 71/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7145 - accuracy: 0.1184\n",
      "dawno dawno temu nie ma nic innego jak w mieście i nie było w tym wypadku i nie było ani czasu ani czasu\n",
      "7276/7276 [==============================] - 1231s 169ms/step - loss: 6.7145 - accuracy: 0.1184\n",
      "Epoch 72/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.7047 - accuracy: 0.1182\n",
      "dawno dawno temu nie ma w tym względzie i nie ma nic innego jak w tym względzie nie mogę się dowiedzieć o tym\n",
      "7276/7276 [==============================] - 1218s 167ms/step - loss: 6.7047 - accuracy: 0.1182\n",
      "Epoch 73/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6982 - accuracy: 0.1190\n",
      "dawno dawno temu nie ma w domu i to samo i że nie ma w tym że nie ma rady i nie było\n",
      "7276/7276 [==============================] - 1217s 167ms/step - loss: 6.6982 - accuracy: 0.1190\n",
      "Epoch 74/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6846 - accuracy: 0.1188\n",
      "dawno dawno temu nie będzie nie wiedział co robić że nie wiem co się stało i nie będzie nie się nie mylę nie\n",
      "7276/7276 [==============================] - 1222s 168ms/step - loss: 6.6846 - accuracy: 0.1188\n",
      "Epoch 75/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6852 - accuracy: 0.1189\n",
      "dawno dawno temu nie ma co robić do domu i nie ma co robić do domu i nie ma nic więcej do mnie\n",
      "7276/7276 [==============================] - 1226s 168ms/step - loss: 6.6852 - accuracy: 0.1189\n",
      "Epoch 76/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6677 - accuracy: 0.1215\n",
      "dawno dawno temu nie ma w nim i nie mogę się z nim bawić i nie ma nikogo nie ma ani innych rzeczy\n",
      "7276/7276 [==============================] - 1227s 169ms/step - loss: 6.6677 - accuracy: 0.1215\n",
      "Epoch 77/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6809 - accuracy: 0.1201\n",
      "dawno dawno temu nie ma w nim i i nie było w domu i się w tym że nie było w domu i\n",
      "7276/7276 [==============================] - 1229s 169ms/step - loss: 6.6809 - accuracy: 0.1201\n",
      "Epoch 78/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6728 - accuracy: 0.1204\n",
      "dawno dawno temu nie mogę się z nim obawiać że nie ma w świecie i nie było w tym celu i w tym\n",
      "7276/7276 [==============================] - 1230s 169ms/step - loss: 6.6728 - accuracy: 0.1204\n",
      "Epoch 79/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6776 - accuracy: 0.1209\n",
      "dawno dawno temu nie ma w tym i nie się ani słowa nie będzie nie do niej nie będzie się z nimi i\n",
      "7276/7276 [==============================] - 1227s 169ms/step - loss: 6.6776 - accuracy: 0.1209\n",
      "Epoch 80/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6640 - accuracy: 0.1206\n",
      "dawno dawno temu nie mógł się powstrzymać z sobą i nie mógł się przekonać czy nie nie miał żadnego wahania do niego i\n",
      "7276/7276 [==============================] - 1223s 168ms/step - loss: 6.6640 - accuracy: 0.1206\n",
      "Epoch 81/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6551 - accuracy: 0.1206\n",
      "dawno dawno temu nie ma nic więcej do powiedzenia że nie ma czasu do niczego się nie stanie i nie może być w\n",
      "7276/7276 [==============================] - 1228s 169ms/step - loss: 6.6551 - accuracy: 0.1206\n",
      "Epoch 82/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6504 - accuracy: 0.1217\n",
      "dawno dawno temu nie ma nic innego jak i i nie było w tym wypadku i nie było w tym wypadku i nie\n",
      "7276/7276 [==============================] - 1224s 168ms/step - loss: 6.6504 - accuracy: 0.1217\n",
      "Epoch 83/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6417 - accuracy: 0.1229\n",
      "dawno dawno temu nie mogę się uspokoić i w tym razie nie mogę się z panem pomówić z panem zagłobą z panem zagłobą\n",
      "7276/7276 [==============================] - 1231s 169ms/step - loss: 6.6417 - accuracy: 0.1229\n",
      "Epoch 84/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6313 - accuracy: 0.1216\n",
      "dawno dawno temu nie mógł się powstrzymać z sobą z sobą i się z sobą z sobą i nie mógł się spodziewać że\n",
      "7276/7276 [==============================] - 1232s 169ms/step - loss: 6.6313 - accuracy: 0.1216\n",
      "Epoch 85/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6326 - accuracy: 0.1223\n",
      "dawno dawno temu nie ma w tym niż to nie było w tym razie że nie ma w tej chwili że nie ma\n",
      "7276/7276 [==============================] - 1233s 169ms/step - loss: 6.6326 - accuracy: 0.1223\n",
      "Epoch 86/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6349 - accuracy: 0.1229\n",
      "dawno dawno temu nie ma co robić o tym że nie ma nic innego jak najprędzej i do czynienia z i o tym\n",
      "7276/7276 [==============================] - 1232s 169ms/step - loss: 6.6349 - accuracy: 0.1229\n",
      "Epoch 87/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6176 - accuracy: 0.1238\n",
      "dawno dawno temu nie wiedział co się stało z tobą i nie mogę się z nimi dowiedzieć czy nie mogę się z nim\n",
      "7276/7276 [==============================] - 1217s 167ms/step - loss: 6.6176 - accuracy: 0.1238\n",
      "Epoch 88/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6300 - accuracy: 0.1228\n",
      "dawno dawno temu nie się nie do tego że nie ma co do powiedzenia do licha nie wiem co się stało i nie\n",
      "7276/7276 [==============================] - 1213s 167ms/step - loss: 6.6300 - accuracy: 0.1228\n",
      "Epoch 89/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6158 - accuracy: 0.1237\n",
      "dawno dawno temu nie mogę się z nią z sobą z sobą i nie było w tym stopniu nie było do roboty i\n",
      "7276/7276 [==============================] - 1230s 169ms/step - loss: 6.6158 - accuracy: 0.1237\n",
      "Epoch 90/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6139 - accuracy: 0.1255\n",
      "dawno dawno temu nie mogę się z nimi spodziewać że nie ma na świecie i nie było już na świecie i nie było\n",
      "7276/7276 [==============================] - 1226s 168ms/step - loss: 6.6139 - accuracy: 0.1255\n",
      "Epoch 91/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6073 - accuracy: 0.1237\n",
      "dawno dawno temu nie wiem czy nie ma nic innego jak najprędzej i do niego nie będzie nie się z nami i nie\n",
      "7276/7276 [==============================] - 1231s 169ms/step - loss: 6.6073 - accuracy: 0.1237\n",
      "Epoch 92/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.6007 - accuracy: 0.1243\n",
      "dawno dawno temu nie się w tym że nie było w tym razem że nie się na niego i nie było ani śladu\n",
      "7276/7276 [==============================] - 1231s 169ms/step - loss: 6.6007 - accuracy: 0.1243\n",
      "Epoch 93/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.5963 - accuracy: 0.1246\n",
      "dawno dawno temu nie mogę wierzyć i nie było żadnego nadziei że nie ma co do tego co się stało nie mogę wierzyć\n",
      "7276/7276 [==============================] - 1235s 170ms/step - loss: 6.5963 - accuracy: 0.1246\n",
      "Epoch 94/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.5788 - accuracy: 0.1254\n",
      "dawno dawno temu nie ma się bać i i nie mogę się przekonać czy nie ma nic innego jak na to nie mogę\n",
      "7276/7276 [==============================] - 1232s 169ms/step - loss: 6.5788 - accuracy: 0.1254\n",
      "Epoch 95/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.5817 - accuracy: 0.1259\n",
      "dawno dawno temu nie wiem co się stało i nie było w tej chwili że nie ma nic więcej niż to nie było\n",
      "7276/7276 [==============================] - 1235s 170ms/step - loss: 6.5817 - accuracy: 0.1259\n",
      "Epoch 96/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.5868 - accuracy: 0.1249\n",
      "dawno dawno temu nie będzie w tym słowie i nie było w tym względzie to nie było w tym że nie było w\n",
      "7276/7276 [==============================] - 1236s 170ms/step - loss: 6.5868 - accuracy: 0.1249\n",
      "Epoch 97/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.5944 - accuracy: 0.1255\n",
      "dawno dawno temu nie się w tym życiu i nie było to w tym miejscu i nie było ani śladu i w tym\n",
      "7276/7276 [==============================] - 1233s 169ms/step - loss: 6.5944 - accuracy: 0.1255\n",
      "Epoch 98/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.5828 - accuracy: 0.1261\n",
      "dawno dawno temu nie ma tego czasu do stracenia odpowiedział stary kiemlicz z bogdańca i i nie wiedząc co się stało z tobą\n",
      "7276/7276 [==============================] - 1235s 170ms/step - loss: 6.5828 - accuracy: 0.1261\n",
      "Epoch 99/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.5664 - accuracy: 0.1268\n",
      "dawno dawno temu nie ma w tej chwili że nie ma w tej chwili że nie ma w tej chwili że nie ma\n",
      "7276/7276 [==============================] - 1237s 170ms/step - loss: 6.5664 - accuracy: 0.1268\n",
      "Epoch 100/100\n",
      "7276/7276 [==============================] - ETA: 0s - loss: 6.5720 - accuracy: 0.1260\n",
      "dawno dawno temu nie się w domu i nie było żadnej wątpliwości że nie ma żadnej rady i nie było żadnej wątpliwości że\n",
      "7276/7276 [==============================] - 1224s 168ms/step - loss: 6.5720 - accuracy: 0.1260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2023ee74dc0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generator(batch_size):\n",
    "    while True:\n",
    "        batch_contexts = []\n",
    "        batch_targets = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            idx = np.random.randint(len(tokenized_sentences))\n",
    "            sentence = tokenized_sentences[idx]\n",
    "            id_sentence = [word_to_id[word] for word in sentence if word in word_to_id]\n",
    "\n",
    "            split_idx = np.random.randint(0, min(len(id_sentence), max_sentence_length))\n",
    "\n",
    "            context = id_sentence[:split_idx]\n",
    "            target = id_sentence[split_idx]\n",
    "\n",
    "            context = context + [word_to_id['<PAD>']] * (max_sentence_length - len(context))\n",
    "            \n",
    "            batch_contexts.append(context)\n",
    "            batch_targets.append(target)\n",
    "\n",
    "        yield np.array(batch_contexts), to_categorical(batch_targets, num_classes=len(word_to_id))\n",
    "\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "data_generator = generator(batch_size)\n",
    "\n",
    "model.fit(data_generator, steps_per_epoch=len(tokenized_sentences) // batch_size, callbacks=[checkpoint_callback, CustomCallback()], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model_checkpoint_13.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dawno dawno temu nie wiedział co począć z tego nie było w tym porządku że nie było w tym rzeczach i na razie nie było ani słowa ani słowa ani ani ani ani ani oka nie nie nie na to łzach pojmujecie wolnej\n"
     ]
    }
   ],
   "source": [
    "sentence = ['dawno', 'dawno', 'temu']  # Assuming you have tokenized_sentences defined\n",
    "seed_context = [word_to_id[word] for word in sentence if word in word_to_id]\n",
    "\n",
    "max_sentence_length = 40\n",
    "\n",
    "num_words_to_generate = 40  # Adjust as needed\n",
    "\n",
    "sentence_added = 'dawno dawno temu'\n",
    "for _ in range(num_words_to_generate):\n",
    "    padded_context = np.array([seed_context + [word_to_id['<PAD>']] * (max_sentence_length - len(seed_context))])\n",
    "    predicted_probs = model.predict(padded_context, verbose=0)\n",
    "    predicted_word_id = np.argmax(predicted_probs)\n",
    "    predicted_word = id_to_word[predicted_word_id]  # Assuming you have id_to_word mapping\n",
    "    sentence_added += \" \" + predicted_word\n",
    "    seed_context.append(predicted_word_id)\n",
    "    seed_context = seed_context[-max_sentence_length:]  # Keep context within max_sentence_length\n",
    "print()\n",
    "print(sentence_added)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
