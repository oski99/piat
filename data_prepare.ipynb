{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import os\n",
    "import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [00:14<00:00,  6.12it/s] \n"
     ]
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    # Remove punctuation\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation and '\\n' not in v)\n",
    "    \n",
    "    # Remove non-alphanumeric characters (including special characters)\n",
    "    txt = re.sub(r'[^a-zA-Ząćęłńóśźż\\s]', '', txt)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    return txt\n",
    "\n",
    "curr_dir = 'bajki/'\n",
    "sentences = []\n",
    "\n",
    "for filename in tqdm.tqdm(os.listdir(curr_dir)):\n",
    "    with open(curr_dir + filename, 'r', encoding='utf-8') as f:\n",
    "        tale_text = f.read()\n",
    "        tale_text = sent_tokenize(tale_text)\n",
    "\n",
    "        sentences.extend([clean_text(l) for l in tale_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pewna uboga wdowa mieszkała w samotnej chatce a przed tą chatką był ogródek w którym rosły dwa krzaki róż jedne róże białe drugie czerwone',\n",
       " 'wdowa miała dwie córeczki podobne do obu krzaków róż',\n",
       " 'jedna zwała się białośnieżką a druga różanką',\n",
       " 'dziewczątka były tak pobożne i dobre tak pracowite i roztropne że chyba drugich takich dzieci nie było na świecie ale białośnieżka była łagodniejszą niż różanka',\n",
       " 'różanka wolała skakać po łąkach i polach rwała kwiatki i goniła ptaszyny leśne a białośnieżka siadywała w domu przy matce pomagała jej w gospodarstwie albo czytywała jej książki gdy nic innego nie było do roboty',\n",
       " 'obie dziewczynki tak bardzo się kochały że zawsze idąc razem trzymały się za rączki a gdy białośnieżka mówiła my się nigdy nie rozstaniemy różanka odpowiadała tym co ma jedna musi się i druga podzielićnieraz biegały same po lesie i zbierały czerwone jagody żadne zwierzę nie robiło im nic złego ale przychodziły do nich z całym zaufaniem zajączek jadł z ich rąk kapustę sarna skubała trawkę tuż przy nich jeleń wysoko wyskakiwał obok ptaki zostawały na gałęziach i wyśpiewywały wszystkie swoje trele',\n",
       " 'dziewczęta nie miewały żadnych przygód jeżeli się zapóźniły w lesie a noc je zaskoczyła to kładły się jedna przy drugiej na murawie i zasypiały aż do ranka a matka wiedziała o tym i nie trwożyła się o nie',\n",
       " 'pewnego razu gdy nocowały w lesie a świt różany je zbudził spostrzegły śliczne dziecko w białej błyszczącej sukieneczce siedzące przy ich posłaniu i spoglądające na nie bardzo życzliwie',\n",
       " 'nic nie mówiąc odeszło potem w głąb lasu a gdy się obejrzały zauważyły że leżą tuż nad głęboką przepaścią że gdyby były w ciemnościach i stąpiły jeszcze parę kroków to byłyby w tę przepaść wpadły',\n",
       " 'a matka im powiedziała że musiał to być anioł stróż który czuwa nad dobrymi dziećmi']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences into words\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "tokenized_sentences = [sentence for sentence in tokenized_sentences if len([sentence]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the tokenized sentences into a single list of words\n",
    "all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "\n",
    "# Create a vocabulary with a limit of 50,000 words\n",
    "word_counts = Counter(all_words)\n",
    "used_words = word_counts.most_common(50000)\n",
    "sorted_vocab = [word for word, _ in used_words]\n",
    "\n",
    "# Create a mapping from words to unique integer IDs\n",
    "word_to_id = {word: idx + 1 for idx, word in enumerate(sorted_vocab)}\n",
    "\n",
    "# Add a special token for padding (if needed)\n",
    "pad_token = '<PAD>'\n",
    "word_to_id[pad_token] = 0\n",
    "\n",
    "# make id_to_word mapping\n",
    "id_to_word = {idx: word for word, idx in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word_to_id mapping to a file\n",
    "with open('word_to_id_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(word_to_id, f)\n",
    "\n",
    "print(\"Vocabulary mapping saved.\")\n",
    "\n",
    "# Save the id_to_word mapping to a file\n",
    "with open('id_to_word_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(id_to_word, f)\n",
    "\n",
    "# Save the word_to_id mapping to a file\n",
    "with open('sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)\n",
    "\n",
    "# Save the word_to_id mapping to a file\n",
    "with open('tokenized_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_sentences, f)\n",
    "\n",
    "print(\"Sentences saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word_to_id mapping from a file\n",
    "with open('word_to_id_mapping.pkl', 'rb') as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "\n",
    "# load the id_to_word mapping from a file\n",
    "with open('id_to_word_mapping.pkl', 'rb') as f:\n",
    "    id_to_word = pickle.load(f)\n",
    "    \n",
    "# load the tokenized sentences from a file\n",
    "with open('tokenized_sentences.pkl', 'rb') as f:\n",
    "    tokenized_sentences = pickle.load(f)\n",
    "\n",
    "# load the sentences from a file\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
