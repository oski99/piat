{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPRM6Nq2fsUf",
        "outputId": "efafbff8-c910-450d-a2b0-91211028761f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezsMS9FCgpKm"
      },
      "outputs": [],
      "source": [
        "path_tales = 'bajki/'\n",
        "\n",
        "texts = ''\n",
        "for index, path in enumerate(os.listdir(path_tales)):\n",
        "    text = ''\n",
        "    with open(path_tales + path, encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        texts += text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybwLPQsfkkJY",
        "outputId": "594d4130-32b9-42fe-f231-c7139c4dfbde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "100%|██████████| 85/85 [00:12<00:00,  6.82it/s]\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def clean_text(txt):\n",
        "    # Remove punctuation\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation and '\\n' not in v)\n",
        "\n",
        "    # Remove non-alphanumeric characters (including special characters)\n",
        "    txt = re.sub(r'[^a-zA-Ząćęłńóśźż\\s]', '', txt)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    txt = txt.lower()\n",
        "\n",
        "    return txt\n",
        "\n",
        "path_tales = 'bajki/'\n",
        "sentences = []\n",
        "\n",
        "for filename in tqdm.tqdm(os.listdir(path_tales)):\n",
        "    with open(path_tales + filename, 'r', encoding='utf-8') as f:\n",
        "        tale_text = f.read()\n",
        "        tale_text = sent_tokenize(tale_text)\n",
        "\n",
        "        sentences.extend([clean_text(l) for l in tale_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1DiiWAUjMHD",
        "outputId": "093502db-c08b-4a25-d9c2-ff393f32579e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['pewna uboga wdowa mieszkała w samotnej chatce a przed tą chatką był ogródek w którym rosły dwa krzaki róż jedne róże białe drugie czerwone',\n",
              " 'wdowa miała dwie córeczki podobne do obu krzaków róż',\n",
              " 'jedna zwała się białośnieżką a druga różanką',\n",
              " 'dziewczątka były tak pobożne i dobre tak pracowite i roztropne że chyba drugich takich dzieci nie było na świecie ale białośnieżka była łagodniejszą niż różanka',\n",
              " 'różanka wolała skakać po łąkach i polach rwała kwiatki i goniła ptaszyny leśne a białośnieżka siadywała w domu przy matce pomagała jej w gospodarstwie albo czytywała jej książki gdy nic innego nie było do roboty',\n",
              " 'obie dziewczynki tak bardzo się kochały że zawsze idąc razem trzymały się za rączki a gdy białośnieżka mówiła my się nigdy nie rozstaniemy różanka odpowiadała tym co ma jedna musi się i druga podzielićnieraz biegały same po lesie i zbierały czerwone jagody żadne zwierzę nie robiło im nic złego ale przychodziły do nich z całym zaufaniem zajączek jadł z ich rąk kapustę sarna skubała trawkę tuż przy nich jeleń wysoko wyskakiwał obok ptaki zostawały na gałęziach i wyśpiewywały wszystkie swoje trele',\n",
              " 'dziewczęta nie miewały żadnych przygód jeżeli się zapóźniły w lesie a noc je zaskoczyła to kładły się jedna przy drugiej na murawie i zasypiały aż do ranka a matka wiedziała o tym i nie trwożyła się o nie',\n",
              " 'pewnego razu gdy nocowały w lesie a świt różany je zbudził spostrzegły śliczne dziecko w białej błyszczącej sukieneczce siedzące przy ich posłaniu i spoglądające na nie bardzo życzliwie',\n",
              " 'nic nie mówiąc odeszło potem w głąb lasu a gdy się obejrzały zauważyły że leżą tuż nad głęboką przepaścią że gdyby były w ciemnościach i stąpiły jeszcze parę kroków to byłyby w tę przepaść wpadły',\n",
              " 'a matka im powiedziała że musiał to być anioł stróż który czuwa nad dobrymi dziećmi']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5Pr50rsjVnk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHIWyJD5mAzo"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Tokenize the sentences and count word frequencies\n",
        "word_counts = Counter()\n",
        "for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    word_counts.update(words)\n",
        "\n",
        "# Select the top 50,000 most frequent words\n",
        "top_words = set(word for word, _ in word_counts.most_common(50000))\n",
        "\n",
        "# Replace the sentences with only the top words\n",
        "new_sentences = []\n",
        "for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    selected_words = [word for word in words if word in top_words]\n",
        "    new_sentences.append(\" \".join(selected_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLq-EtrCnCCn",
        "outputId": "6b6f9a25-0005-4b35-a4f9-82588b564c4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['widzę nasze chorągwie',\n",
              " 'mogłeś pan sam odgadnąć',\n",
              " 'kto przypadł ten sam sobie do woli czerpał z kadzi bo w taki dzień każdy gościem był miłym',\n",
              " 'zali wytrzymamy do godziny modlitw o głodzie i kto nas później pożywi',\n",
              " 'osiołek nasz dobrze się miewa',\n",
              " 'fortuna i inne nie mogło w żaden sposób np',\n",
              " 'o tym co było zewnątrz ciebie ludzie czcili lecz twoje owoce',\n",
              " 'więc mury klasztorne naokół jak boki okrętu otoczonego burzą i rozbójnikami',\n",
              " 'sam doktor ma z tobą na pieńku powiadał niewdzięczny kiedy wspominał ciebie',\n",
              " 'nel zachwycała się szczególnie widokiem i czarnych sporych ptaków które odzywały się głosem']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-TPaYNLjWnU"
      },
      "outputs": [],
      "source": [
        "length = len(new_sentences)\n",
        "text_train = new_sentences[:int(0.7*length)]\n",
        "text_test = new_sentences[int(0.7*length):int(0.85*length)]\n",
        "text_valid = new_sentences[int(0.85*length):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPeHUzTCjirR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    sentence = tf.strings.lower(input_string)\n",
        "    sentence = tf.strings.regex_replace(sentence, \"\\n\", \" \")\n",
        "    return sentence\n",
        "\n",
        "maxlen = 50\n",
        "# You can also set calculate the longest sentence in the data - 25 in this case\n",
        "# maxlen = len(max(text_list).split(' '))\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize = custom_standardization,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(new_sentences)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD40WRgJjjXp",
        "outputId": "f871e440-89ab-4ccc-cb02-6ee3dea52e3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50002"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-nvg-UdLj3gm",
        "outputId": "49394815-59d7-4b7a-fe19-759b0d570c3e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'nie'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "index_lookup[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4dkpM97j6dS",
        "outputId": "10c8a151-655e-424c-d88a-eefe7d24b96a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 51), dtype=int64, numpy=\n",
              "array([[923, 923, 323,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorize_layer(['dawno dawno temu'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb4NUT_Hj_j9"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(text_train)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=256)\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(text_test)\n",
        "test_dataset = test_dataset.shuffle(buffer_size=256)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices(text_valid)\n",
        "valid_dataset = valid_dataset.shuffle(buffer_size=256)\n",
        "valid_dataset = valid_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyT1MzkPkDf-"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_text)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = test_dataset.map(preprocess_text)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_dataset = valid_dataset.map(preprocess_text)\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRTnB7NdkGpK",
        "outputId": "9a05c63e-519e-4701-da12-8a9917a6bb8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(64, 50), dtype=int64, numpy=\n",
            "array([[   97,  1507,     3, ...,     0,     0,     0],\n",
            "       [  157,    23,    78, ...,     0,     0,     0],\n",
            "       [   31,    86,  1738, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [ 1558,    90,   469, ...,     0,     0,     0],\n",
            "       [  393,  1179,    42, ...,     0,     0,     0],\n",
            "       [   17,   383, 22407, ...,     0,     0,     0]])>, <tf.Tensor: shape=(64, 50), dtype=int64, numpy=\n",
            "array([[ 1507,     3,     8, ...,     0,     0,     0],\n",
            "       [   23,    78,  6898, ...,     0,     0,     0],\n",
            "       [   86,  1738,    11, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [   90,   469, 12951, ...,     0,     0,     0],\n",
            "       [ 1179,    42,   441, ...,     0,     0,     0],\n",
            "       [  383, 22407,   818, ...,     0,     0,     0]])>)\n"
          ]
        }
      ],
      "source": [
        "for entry in train_dataset.take(1):\n",
        "    print(entry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxRYiT2WkJnu",
        "outputId": "74729d52-7980-42d0-d9e6-9a319e2265c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 50)]              0         \n",
            "                                                                 \n",
            " embedding_layer (TokenAndP  (None, 50, 128)           6406656   \n",
            " ositionEmbedding)                                               \n",
            "                                                                 \n",
            " transformer_decoder (Trans  (None, 50, 128)           99584     \n",
            " formerDecoder)                                                  \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 50, 50002)         6450258   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12956498 (49.43 MB)\n",
            "Trainable params: 12956498 (49.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_nlp\n",
        "\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "\n",
        "def create_model():\n",
        "    inputs = keras.layers.Input(shape=(maxlen,), dtype=tf.int32, name='input_layer')\n",
        "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(vocab_size, maxlen, embed_dim, name='embedding_layer')(inputs)\n",
        "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim,\n",
        "                                                  num_heads=num_heads,\n",
        "                                                  dropout=0.5, name='transformer_decoder')(embedding_layer)\n",
        "\n",
        "    outputs = keras.layers.Dense(vocab_size, activation='softmax', name='output_layer')(decoder)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71yk05YPkcbL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "class TextSampler(keras.callbacks.Callback):\n",
        "    def __init__(self, start_prompt, max_tokens):\n",
        "        self.start_prompt = start_prompt\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    # Helper method to choose a word from the top K probable words with respect to their probabilities\n",
        "    # in a sequence\n",
        "    def sample_token(self, logits):\n",
        "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        decoded_sample = self.start_prompt\n",
        "\n",
        "        for i in range(self.max_tokens-1):\n",
        "            tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n",
        "            predictions = self.model.predict([tokenized_prompt], verbose=0)\n",
        "            # To find the index of the next word in the prediction array.\n",
        "            # The tokenized prompt is already shorter than the original decoded sample\n",
        "            # by one, len(decoded_sample.split()) is two words ahead - so we remove 1 to get\n",
        "            # the next word in the sequence\n",
        "            sample_index = len(decoded_sample.strip().split())-1\n",
        "\n",
        "            sampled_token = self.sample_token(predictions[0][sample_index])\n",
        "            sampled_token = index_lookup[sampled_token]\n",
        "            decoded_sample += \" \" + sampled_token\n",
        "\n",
        "        print(f\"\\nSample text:\\n{decoded_sample}...\\n\")\n",
        "\n",
        "# First 5 words of a random sentence to be used as a seed\n",
        "seed_sentence = 'dawno dawno temu'\n",
        "sampler = TextSampler(seed_sentence, 50)\n",
        "reducelr = keras.callbacks.ReduceLROnPlateau(patience=10, monitor='val_loss')\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath='checkpoints/transformer_{epoch:02d}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgcJpMoqko79",
        "outputId": "69514fd8-df67-4b6d-8cc0-2e33db9ae484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.9391 - perplexity: 6.9524 - accuracy: 0.7705\n",
            "Sample text:\n",
            "dawno dawno temu nie mógł  już z sobą  do nich  w której by mu się na ziemię z nim się do nich  i z nim na świecie ...\n",
            "\n",
            "2672/2672 [==============================] - 605s 223ms/step - loss: 1.9391 - perplexity: 6.9524 - accuracy: 0.7705 - val_loss: 1.7457 - val_perplexity: 5.7299 - val_accuracy: 0.7753 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.6517 - perplexity: 5.2160 - accuracy: 0.7767\n",
            "Sample text:\n",
            "dawno dawno temu temu i z tego nie ma się do niego  w niej w tej nocy gdy mu w drogę  i  tak jest to że to jest ...\n",
            "\n",
            "2672/2672 [==============================] - 548s 205ms/step - loss: 1.6517 - perplexity: 5.2160 - accuracy: 0.7767 - val_loss: 1.7189 - val_perplexity: 5.5783 - val_accuracy: 0.7755 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.5376 - perplexity: 4.6535 - accuracy: 0.7781\n",
            "Sample text:\n",
            "dawno dawno temu że na ten czas   już nie może nie  ma co innego  nie może zdarzyć może się    z nim w razie nie było...\n",
            "\n",
            "2672/2672 [==============================] - 550s 206ms/step - loss: 1.5376 - perplexity: 4.6535 - accuracy: 0.7781 - val_loss: 1.7431 - val_perplexity: 5.7152 - val_accuracy: 0.7743 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.4613 - perplexity: 4.3114 - accuracy: 0.7786\n",
            "Sample text:\n",
            "dawno dawno temu  na  tym  samym i że się na pewno nie było już nie ma w niej ani ani słowa które na to nie  było żadnej rady...\n",
            "\n",
            "2672/2672 [==============================] - 546s 204ms/step - loss: 1.4613 - perplexity: 4.3114 - accuracy: 0.7786 - val_loss: 1.7721 - val_perplexity: 5.8832 - val_accuracy: 0.7734 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.4145 - perplexity: 4.1146 - accuracy: 0.7792\n",
            "Sample text:\n",
            "dawno dawno temu nie może i na to   jest   to co do niego z tego  nie  jest na to co do  mnie do mnie w...\n",
            "\n",
            "2672/2672 [==============================] - 545s 204ms/step - loss: 1.4145 - perplexity: 4.1146 - accuracy: 0.7792 - val_loss: 1.7951 - val_perplexity: 6.0204 - val_accuracy: 0.7731 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.3844 - perplexity: 3.9923 - accuracy: 0.7800\n",
            "Sample text:\n",
            "dawno dawno temu i  nie mogę bo ja się w niej  w niej i na to  nie wiem jak to że jest w tej chwili gdy na to nie...\n",
            "\n",
            "2672/2672 [==============================] - 545s 204ms/step - loss: 1.3844 - perplexity: 3.9923 - accuracy: 0.7800 - val_loss: 1.8145 - val_perplexity: 6.1381 - val_accuracy: 0.7721 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.3626 - perplexity: 3.9062 - accuracy: 0.7808\n",
            "Sample text:\n",
            "dawno dawno temu co do  tego że to ja się z tego listu na niego i na pewno żem cię uczynił i  na śmierć i rzekł  don kichota i...\n",
            "\n",
            "2672/2672 [==============================] - 543s 203ms/step - loss: 1.3626 - perplexity: 3.9062 - accuracy: 0.7808 - val_loss: 1.8326 - val_perplexity: 6.2504 - val_accuracy: 0.7716 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.3459 - perplexity: 3.8417 - accuracy: 0.7816\n",
            "Sample text:\n",
            "dawno dawno temu i na to jest i na śmierć w tym samym  wzgórzem koło ust jej się i  nie tylko że na świecie    a on zaś...\n",
            "\n",
            "2672/2672 [==============================] - 543s 203ms/step - loss: 1.3459 - perplexity: 3.8417 - accuracy: 0.7816 - val_loss: 1.8456 - val_perplexity: 6.3322 - val_accuracy: 0.7721 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.3321 - perplexity: 3.7888 - accuracy: 0.7824\n",
            "Sample text:\n",
            "dawno dawno temu i rzekł  a nie ma w końcu na  świecie   ale w nim  nie może zdarzyć może  i w takim jak w takim jak...\n",
            "\n",
            "2672/2672 [==============================] - 546s 204ms/step - loss: 1.3321 - perplexity: 3.7888 - accuracy: 0.7824 - val_loss: 1.8602 - val_perplexity: 6.4252 - val_accuracy: 0.7721 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.3201 - perplexity: 3.7438 - accuracy: 0.7832\n",
            "Sample text:\n",
            "dawno dawno temu co się w stanie się  z  tej historii    nie  było już i nie może i nie  ma  być na to nie...\n",
            "\n",
            "2672/2672 [==============================] - 545s 204ms/step - loss: 1.3201 - perplexity: 3.7438 - accuracy: 0.7832 - val_loss: 1.8732 - val_perplexity: 6.5094 - val_accuracy: 0.7727 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.3098 - perplexity: 3.7055 - accuracy: 0.7838\n",
            "Sample text:\n",
            "dawno dawno temu co robić w stanie  wyrzec wszystkiego złego nie tylko na świecie  a nie tylko o tym co to  nie jest to   co  to...\n",
            "\n",
            "2672/2672 [==============================] - 543s 203ms/step - loss: 1.3098 - perplexity: 3.7055 - accuracy: 0.7838 - val_loss: 1.8850 - val_perplexity: 6.5861 - val_accuracy: 0.7719 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.3005 - perplexity: 3.6713 - accuracy: 0.7846\n",
            "Sample text:\n",
            "dawno dawno temu i nie  ma ani na tym świecie  a nie ma w tej porze roku  i  że się do domu nie będzie na okręcie płynący czy...\n",
            "\n",
            "2672/2672 [==============================] - 543s 203ms/step - loss: 1.3005 - perplexity: 3.6713 - accuracy: 0.7846 - val_loss: 1.8954 - val_perplexity: 6.6554 - val_accuracy: 0.7720 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.2331 - perplexity: 3.4317 - accuracy: 0.7886\n",
            "Sample text:\n",
            "dawno dawno temu co się dzieje na tym samym miejscu a ja nie chcę  żeby nie tylko dlatego żeby mu było już nie wiem co innego jak i to jest i...\n",
            "\n",
            "2672/2672 [==============================] - 544s 204ms/step - loss: 1.2331 - perplexity: 3.4317 - accuracy: 0.7886 - val_loss: 1.9180 - val_perplexity: 6.8074 - val_accuracy: 0.7740 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.2078 - perplexity: 3.3460 - accuracy: 0.7908\n",
            "Sample text:\n",
            "dawno dawno temu wyruszyłem razem z sobą  w niedzielę około dwudziestu czterech przyjaciół wolnych kmieciów okolicy przez cieśninę torresa jest wyspą czy to bazaltów to na oceanie a może  być...\n",
            "\n",
            "2672/2672 [==============================] - 545s 204ms/step - loss: 1.2078 - perplexity: 3.3460 - accuracy: 0.7908 - val_loss: 1.9317 - val_perplexity: 6.9012 - val_accuracy: 0.7742 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.1968 - perplexity: 3.3095 - accuracy: 0.7918\n",
            "Sample text:\n",
            "dawno dawno temu  co do tego rodzaju objawy nadzwyczajnej mojej powieści które mu było w jego  nie może się w niej z tych samych ostawię a ja sam z was...\n",
            "\n",
            "2672/2672 [==============================] - 544s 204ms/step - loss: 1.1968 - perplexity: 3.3095 - accuracy: 0.7918 - val_loss: 1.9433 - val_perplexity: 6.9821 - val_accuracy: 0.7742 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.1898 - perplexity: 3.2865 - accuracy: 0.7925\n",
            "Sample text:\n",
            "dawno dawno temu winien był na świecie a to i to w nim     w tej chwili kiedy na to się z nim nie  było to że w...\n",
            "\n",
            "2672/2672 [==============================] - 544s 203ms/step - loss: 1.1898 - perplexity: 3.2865 - accuracy: 0.7925 - val_loss: 1.9533 - val_perplexity: 7.0520 - val_accuracy: 0.7742 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.1848 - perplexity: 3.2700 - accuracy: 0.7929\n",
            "Sample text:\n",
            "dawno dawno temu wyruszyłem piechotą przy odgłosie wesołej kompanji bardzo bardzo się z powodu do gotowania i w nocy podczas całej wiosce  z którego w życiu widział jak i nie było...\n",
            "\n",
            "2672/2672 [==============================] - 543s 203ms/step - loss: 1.1848 - perplexity: 3.2700 - accuracy: 0.7929 - val_loss: 1.9621 - val_perplexity: 7.1142 - val_accuracy: 0.7743 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.1806 - perplexity: 3.2562 - accuracy: 0.7933\n",
            "Sample text:\n",
            "dawno dawno temu co się zowie a ja nie umiem a ja się nie będę  mógł i tak jak to być w tej pory roku zeszłym tygodniu  czas jakiś człowiek...\n",
            "\n",
            "2672/2672 [==============================] - 542s 203ms/step - loss: 1.1806 - perplexity: 3.2562 - accuracy: 0.7933 - val_loss: 1.9703 - val_perplexity: 7.1728 - val_accuracy: 0.7742 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "2672/2672 [==============================] - ETA: 0s - loss: 1.1772 - perplexity: 3.2451 - accuracy: 0.7936\n",
            "Sample text:\n",
            "dawno dawno temu i rzekł do siebie  a nie ma  rady     sobie rady  to jest i że na mnie  to że to się do...\n",
            "\n",
            "2672/2672 [==============================] - 540s 202ms/step - loss: 1.1772 - perplexity: 3.2451 - accuracy: 0.7936 - val_loss: 1.9779 - val_perplexity: 7.2277 - val_accuracy: 0.7742 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "2185/2672 [=======================>......] - ETA: 1:23 - loss: 1.1790 - perplexity: 3.2510 - accuracy: 0.7933"
          ]
        }
      ],
      "source": [
        "model = create_model()\n",
        "history = model.fit(train_dataset,\n",
        "                    validation_data=valid_dataset,\n",
        "                    epochs=100,\n",
        "                    callbacks=[sampler, reducelr, checkpoint_callback])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
